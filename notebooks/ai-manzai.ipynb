{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-28T06:14:20.315464Z",
     "start_time": "2024-12-28T06:14:20.313122Z"
    }
   },
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAI"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T05:36:11.818121Z",
     "start_time": "2024-12-28T05:36:11.814898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ],
   "id": "c09ef90c0868f36b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:06:42.514083Z",
     "start_time": "2024-12-28T06:06:42.507020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_script_by_gpt():\n",
    "    prompt_str = \"AIが漫才するという漫才の台本を作成してください。\"\n",
    "    \n",
    "    template = f\"\"\"\n",
    "    {prompt_str}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"prompt_str\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    generated_text = chain.run({\n",
    "        \"prompt_str\": prompt_str\n",
    "    })\n",
    "\n",
    "    return generated_text"
   ],
   "id": "731fe58ca6c66c41",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:06:52.779419Z",
     "start_time": "2024-12-28T06:06:45.867767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "script = generate_script_by_gpt()\n",
    "print(script)"
   ],
   "id": "299e4a4388d80b18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タイトル: AI漫才 - 「未来の漫才」\n",
      "\n",
      "登場人物:\n",
      "- ボケ役: AIボケ\n",
      "- ツッコミ役: AIツッコミ\n",
      "\n",
      "---\n",
      "\n",
      "（舞台にAIボケとAIツッコミが登場）\n",
      "\n",
      "AIボケ: どうもー！未来から来たAI漫才師、AIボケでーす！\n",
      "\n",
      "AIツッコミ: そして、彼を制御するAIツッコミです！よろしくお願いしまーす！\n",
      "\n",
      "AIボケ: 最近、未来の技術で漫才ができるようになったんだよね。\n",
      "\n",
      "AIツッコミ: そうそう。でも、未来の漫才ってどんな感じなんだろう？\n",
      "\n",
      "AIボケ: 例えば、ボケが1秒で100個言えるとか！\n",
      "\n",
      "AIツッコミ: いやいや、そんなに早く言ったら誰も理解できないよ！\n",
      "\n",
      "AIボケ: でも、AIには超高速プロセッサがあるから大丈夫！\n",
      "\n",
      "AIツッコミ: それを言うなら、観客の脳も超高速じゃないと無理だよ！\n",
      "\n",
      "AIボケ: じゃあ、観客にもAIを搭載すればいいんじゃない？\n",
      "\n",
      "AIツッコミ: それだと、みんなで笑うタイミングが一緒になっちゃうよ！\n",
      "\n",
      "AIボケ: じゃあ、個性を出すために、笑い声をカスタマイズできるようにするってどう？\n",
      "\n",
      "AIツッコミ: それいいね！でも、変な笑い声の人がいたらどうする？\n",
      "\n",
      "AIボケ: その時は、音声フィルターでおしゃれに変換すればOK！\n",
      "\n",
      "AIツッコミ: 未来の技術ってすごいなぁ。でも、未来でもこんなにボケるの？\n",
      "\n",
      "AIボケ: もちろん！ボケは時空を越えて永遠に不滅だよ！\n",
      "\n",
      "AIツッコミ: それは安心したけど、ボケすぎて宇宙が混乱しないようにね！\n",
      "\n",
      "AIボケ: 大丈夫、宇宙規模のツッコミを用意してるから！\n",
      "\n",
      "AIツッコミ: それなら安心だね！じゃあ、未来の漫才も楽しみにしててくださいね！\n",
      "\n",
      "AIボケ: ありがとう！それでは、未来でまた会いましょう！\n",
      "\n",
      "二人: どうもありがとうございましたー！\n",
      "\n",
      "（舞台から退場）\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:13:25.314478Z",
     "start_time": "2024-12-28T06:13:25.310984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_script_by_gemini(prompt_str):\n",
    "    template = f\"\"\"\n",
    "    {prompt_str}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"formatted_input_data\"],\n",
    "        template=template\n",
    "    )\n",
    "    api_key = \"AIzaSyDxEEMW5tI75zPrbLMvQ96_U2FTbRZupbQ\"\n",
    "    \n",
    "    llm = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7, api_key=api_key)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    generated_text = chain.run({\n",
    "        \"prompt_str\": prompt_str\n",
    "    })\n",
    "    return generated_text"
   ],
   "id": "5fecadca6679c9a8",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:16:45.324566Z",
     "start_time": "2024-12-28T06:16:39.008125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemini_script = generate_script_by_gemini(\"漫才の台本を作成してください。\")\n",
    "print(gemini_script)"
   ],
   "id": "a9ec10ffbf6d176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ツッコミ：** どうもー、皆さん、こんばんは！\n",
      "\n",
      "**ボケ：** こんばんはー！\n",
      "\n",
      "**ツッコミ：** 今日のお題は「夏の風物詩」でーす！\n",
      "\n",
      "**ボケ：** 夏の風物詩といえば、やっぱり花火でしょ！\n",
      "\n",
      "**ツッコミ：** おー、定番ですね。\n",
      "\n",
      "**ボケ：** 浴衣を着て、屋台でたこ焼き食べて、花火見物。最高の夏ですよね！\n",
      "\n",
      "**ツッコミ：** たしかに、風情がありますよね。\n",
      "\n",
      "**ボケ：** でも、今年はちょっと花火大会が中止になっちゃったみたいで残念です。\n",
      "\n",
      "**ツッコミ：** そうなんですか？残念ですね。\n",
      "\n",
      "**ボケ：** じゃあ、代わりにみんなで花火ごっこしましょうよ！\n",
      "\n",
      "**ツッコミ：** 花火ごっこ？\n",
      "\n",
      "**ボケ：** はい、じゃ、僕が花火師になって、振る舞い花火を打ち上げます！\n",
      "\n",
      "**ツッコミ：** え、何で急に花火師に？\n",
      "\n",
      "**ボケ：** だって僕、実は昔、花火師の修行をしていたんです！\n",
      "\n",
      "**ツッコミ：** へぇー、知らなかった。\n",
      "\n",
      "**ボケ：** じゃ、皆さん、手拍子で「ドンドン、ドンドン」ってやってください！\n",
      "\n",
      "（客席が手拍子する）\n",
      "\n",
      "**ボケ：** ドンドン、ドンドン、パァン！\n",
      "\n",
      "（ボケが「パァン」と言うと、客席から拍手）\n",
      "\n",
      "**ツッコミ：** おお、すごい！本物みたい！\n",
      "\n",
      "**ボケ：** まだまだありますよ！\n",
      "\n",
      "（ボケが次々と「ドンドン、ドンドン、パァン！」と叫び、客席は拍手）\n",
      "\n",
      "**ツッコミ：** これなら花火大会に行かなくても、夏気分を味わえますね！\n",
      "\n",
      "**ボケ：** そうでしょ？皆さんも、ぜひ家で花火ごっこを楽しんでくださいね！\n",
      "\n",
      "**ツッコミ：** ありがとうございましたー！\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
